"use strict";(globalThis.webpackChunk_001_physical_ai_textbook_docs=globalThis.webpackChunk_001_physical_ai_textbook_docs||[]).push([[6181],{702:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"assessments/module5-assessments","title":"Module 5: Robot Learning and Adaptation - Assessments","description":"Section 1: Introduction to Robot Learning","source":"@site/docs/assessments/module5-assessments.md","sourceDirName":"assessments","slug":"/assessments/module5-assessments","permalink":"/Project-Hackathon-I/docs/assessments/module5-assessments","draft":false,"unlisted":false,"editUrl":"https://github.com/MohammadNoman/Project-Hackathon-I/tree/master/frontend/docs/assessments/module5-assessments.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Robot Motion Planning and Control - Assessments","permalink":"/Project-Hackathon-I/docs/assessments/module4-assessments"},"next":{"title":"module6-assessments","permalink":"/Project-Hackathon-I/docs/assessments/module6-assessments"}}');var r=i(4848),o=i(8453);const t={},l="Module 5: Robot Learning and Adaptation - Assessments",a={},c=[{value:"Section 1: Introduction to Robot Learning",id:"section-1-introduction-to-robot-learning",level:2},{value:"Quiz Questions",id:"quiz-questions",level:3},{value:"Project Prompt",id:"project-prompt",level:3},{value:"Section 2: Supervised Learning for Robotics",id:"section-2-supervised-learning-for-robotics",level:2},{value:"Quiz Questions",id:"quiz-questions-1",level:3},{value:"Project Prompt",id:"project-prompt-1",level:3},{value:"Section 3: Unsupervised Learning for Robotics",id:"section-3-unsupervised-learning-for-robotics",level:2},{value:"Quiz Questions",id:"quiz-questions-2",level:3},{value:"Project Prompt",id:"project-prompt-2",level:3},{value:"Section 4: Reinforcement Learning (RL) Fundamentals",id:"section-4-reinforcement-learning-rl-fundamentals",level:2},{value:"Quiz Questions",id:"quiz-questions-3",level:3},{value:"Project Prompt",id:"project-prompt-3",level:3},{value:"Section 5: Model-Free RL Algorithms",id:"section-5-model-free-rl-algorithms",level:2},{value:"Quiz Questions",id:"quiz-questions-4",level:3},{value:"Project Prompt",id:"project-prompt-4",level:3},{value:"Section 6: Model-Based RL Algorithms",id:"section-6-model-based-rl-algorithms",level:2},{value:"Quiz Questions",id:"quiz-questions-5",level:3},{value:"Project Prompt",id:"project-prompt-5",level:3},{value:"Section 7: Imitation Learning / Learning from Demonstration (LfD)",id:"section-7-imitation-learning--learning-from-demonstration-lfd",level:2},{value:"Quiz Questions",id:"quiz-questions-6",level:3},{value:"Project Prompt",id:"project-prompt-6",level:3},{value:"Section 8: Continual and Lifelong Learning",id:"section-8-continual-and-lifelong-learning",level:2},{value:"Quiz Questions",id:"quiz-questions-7",level:3},{value:"Project Prompt",id:"project-prompt-7",level:3},{value:"Section 9: Robot Adaptation",id:"section-9-robot-adaptation",level:2},{value:"Quiz Questions",id:"quiz-questions-8",level:3},{value:"Project Prompt",id:"project-prompt-8",level:3},{value:"Section 10: Challenges in Robot Learning",id:"section-10-challenges-in-robot-learning",level:2},{value:"Quiz Questions",id:"quiz-questions-9",level:3},{value:"Project Prompt",id:"project-prompt-9",level:3},{value:"Section 11: Future Trends",id:"section-11-future-trends",level:2},{value:"Quiz Questions",id:"quiz-questions-10",level:3},{value:"Project Prompt",id:"project-prompt-10",level:3}];function d(e){const n={em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"module-5-robot-learning-and-adaptation---assessments",children:"Module 5: Robot Learning and Adaptation - Assessments"})}),"\n",(0,r.jsx)(n.h2,{id:"section-1-introduction-to-robot-learning",children:"Section 1: Introduction to Robot Learning"}),"\n",(0,r.jsx)(n.h3,{id:"quiz-questions",children:"Quiz Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Multiple Choice:"})," What is the primary reason why robots need to learn in complex, dynamic, and uncertain real-world scenarios?\na) To reduce manufacturing costs\nb) To enable explicit programming for every action\nc) To adapt to unknown environments and perform complex tasks autonomously\nd) To limit human-robot interaction\n",(0,r.jsx)(n.em,{children:"Correct Answer: c)"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Short Answer:"}),' Briefly explain the concept of "catastrophic forgetting" in continual learning for robots.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"True/False:"})," Supervised learning in robotics typically involves the robot finding hidden patterns in unlabeled data.\n",(0,r.jsx)(n.em,{children:"Correct Answer: False"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"project-prompt",children:"Project Prompt"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task:"})," Propose a real-world robotic task (e.g., a service robot in a dynamic office, a rescue robot in a disaster zone, an autonomous farm robot) that would significantly benefit from robot learning."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Describe the chosen robotic task and its environment."}),"\n",(0,r.jsx)(n.li,{children:"Identify at least three reasons why traditional explicit programming would be impractical or inefficient for this task."}),"\n",(0,r.jsx)(n.li,{children:"Suggest which types of robot learning (e.g., Supervised, Unsupervised, Reinforcement, Imitation, Continual, Online Learning) would be most suitable for different aspects of your proposed task, justifying your choices."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"section-2-supervised-learning-for-robotics",children:"Section 2: Supervised Learning for Robotics"}),"\n",(0,r.jsx)(n.h3,{id:"quiz-questions-1",children:"Quiz Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Multiple Choice:"})," Which of the following is an example of a regression task in robotics using supervised learning?\na) Object recognition\nb) Scene classification\nc) Pose estimation (position and orientation)\nd) Action recognition\n",(0,r.jsx)(n.em,{children:"Correct Answer: c)"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Short Answer:"}),' How does "Dataset Aggregation (DAgger)" address the "covariance shift" problem in behavioral cloning?']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Fill in the Blanks:"})," ________ are specifically designed for processing grid-like data like images and are incredibly powerful for object recognition in robotics.\n",(0,r.jsx)(n.em,{children:"Correct Answer: Convolutional Neural Networks (CNNs)"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"project-prompt-1",children:"Project Prompt"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task:"})," Design a supervised learning pipeline for a robot tasked with sorting recyclable materials on a conveyor belt."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Identify the key classification and/or regression tasks involved (e.g., identifying material type, estimating object pose for grasping)."}),"\n",(0,r.jsx)(n.li,{children:"Describe the type of sensor data the robot would collect (e.g., RGB images, depth images)."}),"\n",(0,r.jsx)(n.li,{children:"Outline a strategy for data collection and annotation, considering both manual and potential semi-automatic methods."}),"\n",(0,r.jsx)(n.li,{children:"Suggest appropriate supervised learning algorithms/architectures for each identified task, explaining why they are suitable."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"section-3-unsupervised-learning-for-robotics",children:"Section 3: Unsupervised Learning for Robotics"}),"\n",(0,r.jsx)(n.h3,{id:"quiz-questions-2",children:"Quiz Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Multiple Choice:"})," Which unsupervised learning technique is best suited for identifying distinct types of terrain from lidar scans without explicit guidance?\na) Regression\nb) Classification\nc) Clustering\nd) Behavioral cloning\n",(0,r.jsx)(n.em,{children:"Correct Answer: c)"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Short Answer:"})," Explain how Autoencoders can be used in robotics for dimensionality reduction and feature learning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"True/False:"})," Generative Adversarial Networks (GANs) are primarily used to categorize input data into predefined classes.\n",(0,r.jsx)(n.em,{children:"Correct Answer: False"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"project-prompt-2",children:"Project Prompt"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task:"})," Imagine a robot exploring an unknown environment to build a semantic map. Propose how unsupervised learning techniques could assist in this process."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Describe how clustering could be used to identify recurring environmental features (e.g., walls, doors, specific objects) from raw sensor data (e.g., point clouds, visual features)."}),"\n",(0,r.jsx)(n.li,{children:"Explain how dimensionality reduction (e.g., PCA, Autoencoders) could be applied to simplify high-dimensional sensor data before or during mapping."}),"\n",(0,r.jsx)(n.li,{children:"Discuss how generative models (GANs/VAEs) might be used to enhance the robot's understanding of the environment or for data augmentation."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"section-4-reinforcement-learning-rl-fundamentals",children:"Section 4: Reinforcement Learning (RL) Fundamentals"}),"\n",(0,r.jsx)(n.h3,{id:"quiz-questions-3",children:"Quiz Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Multiple Choice:"}),' In an MDP, what does the "Reward (R)" signal represent?\na) The robot\'s current joint angles\nb) The probability of transitioning to a new state\nc) A scalar feedback indicating how good or bad an action was\nd) The optimal policy for the robot\n',(0,r.jsx)(n.em,{children:"Correct Answer: c)"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Short Answer:"})," Differentiate between a deterministic policy and a stochastic policy in reinforcement learning. When might a stochastic policy be preferred?"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Fill in the Blanks:"})," The core dilemma in RL, balancing trying new actions and using current knowledge for good rewards, is known as the ________ vs. ________ problem.\n",(0,r.jsx)(n.em,{children:"Correct Answer: Exploration, Exploitation"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"project-prompt-3",children:"Project Prompt"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task:"})," Model a simple robot navigation task (e.g., a mobile robot moving to a target in a grid-world) as a Markov Decision Process (MDP)."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Define the States (S), Actions (A), and Rewards (R) for your chosen navigation task."}),"\n",(0,r.jsx)(n.li,{children:"Describe how Transition Probabilities (P(s' | s, a)) would behave in your model (can be simplified, e.g., deterministic or simple stochasticity)."}),"\n",(0,r.jsx)(n.li,{children:"Explain the concepts of State-Value Function V(s) and Action-Value Function Q(s,a) in the context of your navigation task."}),"\n",(0,r.jsx)(n.li,{children:"Discuss how an epsilon-greedy strategy could be applied to balance exploration and exploitation."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"section-5-model-free-rl-algorithms",children:"Section 5: Model-Free RL Algorithms"}),"\n",(0,r.jsx)(n.h3,{id:"quiz-questions-4",children:"Quiz Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Multiple Choice:"}),' What is a key characteristic of Q-learning that makes it "off-policy"?\na) It learns the optimal policy\'s Q-values while following a different behavior policy.\nb) It always follows the optimal policy during training.\nc) It explicitly builds a model of the environment dynamics.\nd) It only works with continuous action spaces.\n',(0,r.jsx)(n.em,{children:"Correct Answer: a)"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Short Answer:"}),' Explain the purpose of "Experience Replay" and "Target Networks" in Deep Q-Networks (DQN).']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"True/False:"})," SARSA is generally considered safer than Q-learning in real-world applications where exploration with sub-optimal actions could be catastrophic.\n",(0,r.jsx)(n.em,{children:"Correct Answer: True"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"project-prompt-4",children:"Project Prompt"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task:"})," Compare and contrast Q-learning and SARSA for a robot learning to balance on two wheels."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Explain how both algorithms would learn the optimal actions (motor commands) to maintain balance."}),"\n",(0,r.jsx)(n.li,{children:'Discuss the implications of Q-learning being "off-policy" and SARSA being "on-policy" for this specific task, particularly concerning safety during the learning phase.'}),"\n",(0,r.jsx)(n.li,{children:"Describe a scenario where one algorithm might be preferred over the other for a real-world balancing robot."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"section-6-model-based-rl-algorithms",children:"Section 6: Model-Based RL Algorithms"}),"\n",(0,r.jsx)(n.h3,{id:"quiz-questions-5",children:"Quiz Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Multiple Choice:"}),' What is the primary advantage of model-based RL algorithms compared to model-free approaches?\na) They are simpler to implement.\nb) They require significantly less real-world interaction (improved sample efficiency).\nc) They are inherently safer during exploration.\nd) They do not suffer from the "sim-to-real gap."\n',(0,r.jsx)(n.em,{children:"Correct Answer: b)"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Short Answer:"})," Briefly explain how Model Predictive Control (MPC) uses a learned dynamics model to control a robot."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"True/False:"})," Model inaccuracies are a minor concern in model-based RL, as the learned policies are robust to discrepancies.\n",(0,r.jsx)(n.em,{children:"Correct Answer: False"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"project-prompt-5",children:"Project Prompt"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task:"})," Design a model-based RL approach for a robotic arm learning to pick and place various objects."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Describe how the robot would learn a system dynamics model (P(s'|s,a) and R(s,a,s')) using neural networks, specifying the inputs and outputs of this network."}),"\n",(0,r.jsx)(n.li,{children:"Explain how this learned model could be used for planning, specifically mentioning either Model Predictive Control (MPC) or Monte Carlo Tree Search (MCTS) in the context of the pick-and-place task."}),"\n",(0,r.jsx)(n.li,{children:"Discuss the potential advantages (e.g., sample efficiency) and disadvantages (e.g., model inaccuracies) of this approach for the robotic arm."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"section-7-imitation-learning--learning-from-demonstration-lfd",children:"Section 7: Imitation Learning / Learning from Demonstration (LfD)"}),"\n",(0,r.jsx)(n.h3,{id:"quiz-questions-6",children:"Quiz Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Multiple Choice:"})," Behavioral cloning frames imitation learning as what type of machine learning problem?\na) Unsupervised learning\nb) Reinforcement learning\nc) Supervised learning\nd) Continual learning\n",(0,r.jsx)(n.em,{children:"Correct Answer: c)"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Short Answer:"})," What is the main difference in approach between Behavioral Cloning and Inverse Reinforcement Learning (IRL) when learning from expert demonstrations?"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"True/False:"}),' DAgger helps to overcome the "covariance shift" problem by iteratively collecting new states encountered by the robot and having an expert provide correct actions for them.\n',(0,r.jsx)(n.em,{children:"Correct Answer: True"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"project-prompt-6",children:"Project Prompt"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task:"})," Propose an imitation learning solution for a humanoid robot learning to perform a complex dance move demonstrated by a human."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Describe the process of collecting human demonstrations, specifying the type of data that would be recorded by the robot (e.g., joint angles, end-effector poses, visual data)."}),"\n",(0,r.jsx)(n.li,{children:"Explain how behavioral cloning would be used to train a policy for the dance move."}),"\n",(0,r.jsx)(n.li,{children:"Discuss the limitations of basic behavioral cloning in this context and how DAgger could be applied to improve the robot's performance and generalization for the dance move."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"section-8-continual-and-lifelong-learning",children:"Section 8: Continual and Lifelong Learning"}),"\n",(0,r.jsx)(n.h3,{id:"quiz-questions-7",children:"Quiz Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Multiple Choice:"}),' What phenomenon does "Elastic Weight Consolidation (EWC)" aim to mitigate in continual learning?\na) The sim-to-real gap\nb) Catastrophic forgetting\nc) Sample inefficiency\nd) The credit assignment problem\n',(0,r.jsx)(n.em,{children:"Correct Answer: b)"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Short Answer:"}),' Explain the difference between "transfer learning" and "multitask learning" in the context of knowledge transfer for robots.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"True/False:"})," Progressive Neural Networks lead to a growing network size but effectively prevent catastrophic forgetting.\n",(0,r.jsx)(n.em,{children:"Correct Answer: True"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"project-prompt-7",children:"Project Prompt"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task:"})," Design a continual learning strategy for a domestic service robot that learns new tasks over its lifespan without forgetting previously acquired skills."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Identify at least three distinct tasks the robot might learn sequentially (e.g., vacuuming, dishwashing, fetching objects)."}),"\n",(0,r.jsx)(n.li,{children:"Explain the challenge of catastrophic forgetting in this scenario."}),"\n",(0,r.jsx)(n.li,{children:"Propose and justify the use of one or more continual learning architectures or strategies (e.g., EWC, rehearsal-based methods, progressive neural networks) to enable the robot to adapt to new tasks while retaining old ones."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"section-9-robot-adaptation",children:"Section 9: Robot Adaptation"}),"\n",(0,r.jsx)(n.h3,{id:"quiz-questions-8",children:"Quiz Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Multiple Choice:"})," Which classical algorithm is widely used for online state estimation in noisy environments by fusing data from various sensors?\na) Support Vector Machines (SVMs)\nb) Kalman Filters\nc) Q-learning\nd) Principal Component Analysis (PCA)\n",(0,r.jsx)(n.em,{children:"Correct Answer: b)"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Short Answer:"})," How can parameter adaptation help a robot deal with component degradation or wear and tear over time? Provide a specific example."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"True/False:"})," Self-calibration allows a robot to correct for misalignments or drifts in its sensors and kinematics over time.\n",(0,r.jsx)(n.em,{children:"Correct Answer: True"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"project-prompt-8",children:"Project Prompt"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task:"})," A factory robot experiences wear and tear in its manipulator arm, causing its movements to become less precise. Design an adaptation strategy for this robot."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Describe how online learning mechanisms could be used to detect and compensate for the component degradation."}),"\n",(0,r.jsx)(n.li,{children:"Explain how parameter adaptation could adjust the robot's control parameters to restore precision."}),"\n",(0,r.jsx)(n.li,{children:"Discuss the role of self-calibration in this scenario, specifying what aspects of the robot might need recalibration and how it could be achieved."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"section-10-challenges-in-robot-learning",children:"Section 10: Challenges in Robot Learning"}),"\n",(0,r.jsx)(n.h3,{id:"quiz-questions-9",children:"Quiz Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Multiple Choice:"}),' What is the "sim-to-real gap" in robot learning?\na) The time delay between receiving a reward and updating the policy.\nb) The discrepancy between simulated and real-world environments.\nc) The challenge of balancing exploration and exploitation.\nd) The inability of robots to generalize to novel situations.\n',(0,r.jsx)(n.em,{children:"Correct Answer: b)"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Short Answer:"}),' Explain how "domain randomization" helps in bridging the sim-to-real gap.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"True/False:"}),' The "credit assignment problem" is particularly challenging in long-horizon robotic tasks due to sparse and delayed rewards.\n',(0,r.jsx)(n.em,{children:"Correct Answer: True"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"project-prompt-9",children:"Project Prompt"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task:"})," Discuss the key challenges in deploying a reinforcement learning-based autonomous delivery robot in a complex urban environment."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'Address the challenge of "sample efficiency" and propose strategies (e.g., synthetic data, data augmentation) to mitigate it.'}),"\n",(0,r.jsx)(n.li,{children:'Explain the "sim-to-real gap" in this context and suggest techniques (e.g., domain randomization) to bridge it.'}),"\n",(0,r.jsx)(n.li,{children:'Discuss the critical "safety" considerations for such a robot interacting with pedestrians and vehicles, and propose approaches for ensuring safe exploration.'}),"\n",(0,r.jsx)(n.li,{children:'Analyze the "generalization" requirements for the robot to perform well in varying weather, lighting, and traffic conditions.'}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"section-11-future-trends",children:"Section 11: Future Trends"}),"\n",(0,r.jsx)(n.h3,{id:"quiz-questions-10",children:"Quiz Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Multiple Choice:"}),' What is the primary goal of "foundation models" in robotics?\na) To create smaller, highly specialized models for individual tasks.\nb) To learn a broad range of fundamental robotic skills and representations from vast, diverse datasets.\nc) To exclusively use supervised learning for all robotic tasks.\nd) To eliminate the need for human-robot interaction.\n',(0,r.jsx)(n.em,{children:"Correct Answer: b)"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Short Answer:"}),' How does "meta-learning" contribute to "few-shot learning" for robots?']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"True/False:"})," Explainable AI (XAI) in robotics primarily focuses on developing methods to make robot decisions opaque and uninterpretable to humans for security reasons.\n",(0,r.jsx)(n.em,{children:"Correct Answer: False"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"project-prompt-10",children:"Project Prompt"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Task:"})," Envision a future where human-robot co-learning is ubiquitous in household settings. Describe how this might work and the ethical considerations that arise."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'Describe scenarios where humans and robots would engage in "interactive learning" and "shared autonomy" in a household. Provide specific examples.'}),"\n",(0,r.jsx)(n.li,{children:'Discuss how "Explainable AI" would be crucial for trust and effective collaboration in these scenarios.'}),"\n",(0,r.jsx)(n.li,{children:'Analyze at least three major "ethical considerations" (e.g., privacy, accountability, job displacement, human-robot relationship) that would need to be addressed as these systems become more prevalent.'}),"\n",(0,r.jsx)(n.li,{children:"Propose potential mitigation strategies or policy considerations for these ethical challenges."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>l});var s=i(6540);const r={},o=s.createContext(r);function t(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);