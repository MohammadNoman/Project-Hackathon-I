"use strict";(globalThis.webpackChunk_001_physical_ai_textbook_docs=globalThis.webpackChunk_001_physical_ai_textbook_docs||[]).push([[9157],{1012:(n,i,e)=>{e.r(i),e.d(i,{assets:()=>t,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>l,toc:()=>d});const l=JSON.parse('{"id":"module5-robot-learning-and-adaptation/outline","title":"Module 5: Robot Learning and Adaptation","description":"1. Introduction to Robot Learning","source":"@site/docs/module5-robot-learning-and-adaptation/outline.md","sourceDirName":"module5-robot-learning-and-adaptation","slug":"/module5-robot-learning-and-adaptation/outline","permalink":"/Project-Hackathon-I/docs/module5-robot-learning-and-adaptation/outline","draft":false,"unlisted":false,"editUrl":"https://github.com/MohammadNoman/Project-Hackathon-I/tree/master/frontend/docs/module5-robot-learning-and-adaptation/outline.md","tags":[],"version":"current","frontMatter":{}}');var r=e(4848),s=e(8453);const o={},a="Module 5: Robot Learning and Adaptation",t={},d=[{value:"1. Introduction to Robot Learning",id:"1-introduction-to-robot-learning",level:2},{value:"2. Supervised Learning for Robotics",id:"2-supervised-learning-for-robotics",level:2},{value:"3. Unsupervised Learning for Robotics",id:"3-unsupervised-learning-for-robotics",level:2},{value:"4. Reinforcement Learning (RL) Fundamentals",id:"4-reinforcement-learning-rl-fundamentals",level:2},{value:"5. Model-Free RL Algorithms",id:"5-model-free-rl-algorithms",level:2},{value:"6. Model-Based RL Algorithms",id:"6-model-based-rl-algorithms",level:2},{value:"7. Imitation Learning / Learning from Demonstration (LfD)",id:"7-imitation-learning--learning-from-demonstration-lfd",level:2},{value:"8. Continual and Lifelong Learning",id:"8-continual-and-lifelong-learning",level:2},{value:"9. Robot Adaptation",id:"9-robot-adaptation",level:2},{value:"10. Challenges in Robot Learning",id:"10-challenges-in-robot-learning",level:2},{value:"11. Future Trends",id:"11-future-trends",level:2}];function c(n){const i={h1:"h1",h2:"h2",header:"header",li:"li",ul:"ul",...(0,s.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.header,{children:(0,r.jsx)(i.h1,{id:"module-5-robot-learning-and-adaptation",children:"Module 5: Robot Learning and Adaptation"})}),"\n",(0,r.jsx)(i.h2,{id:"1-introduction-to-robot-learning",children:"1. Introduction to Robot Learning"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:["Why robots need to learn","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Adapting to unknown environments"}),"\n",(0,r.jsx)(i.li,{children:"Performing complex tasks"}),"\n",(0,r.jsx)(i.li,{children:"Improving performance over time"}),"\n",(0,r.jsx)(i.li,{children:"Reducing manual programming effort"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Types of Learning in Robotics","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Supervised Learning"}),"\n",(0,r.jsx)(i.li,{children:"Unsupervised Learning"}),"\n",(0,r.jsx)(i.li,{children:"Reinforcement Learning"}),"\n",(0,r.jsx)(i.li,{children:"Imitation Learning"}),"\n",(0,r.jsx)(i.li,{children:"Continual/Lifelong Learning"}),"\n",(0,r.jsx)(i.li,{children:"Online Learning"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"2-supervised-learning-for-robotics",children:"2. Supervised Learning for Robotics"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:["Classification Tasks","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Object recognition"}),"\n",(0,r.jsx)(i.li,{children:"Scene classification"}),"\n",(0,r.jsx)(i.li,{children:"Action recognition"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Regression Tasks","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Pose estimation (position and orientation)"}),"\n",(0,r.jsx)(i.li,{children:"Force/torque prediction"}),"\n",(0,r.jsx)(i.li,{children:"Trajectory prediction"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Data Collection and Annotation","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Sensor data (vision, lidar, tactile)"}),"\n",(0,r.jsx)(i.li,{children:"Human demonstrations"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Common Algorithms and Architectures","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Support Vector Machines (SVMs)"}),"\n",(0,r.jsx)(i.li,{children:"Decision Trees and Random Forests"}),"\n",(0,r.jsx)(i.li,{children:"Neural Networks (Multilayer Perceptrons, Convolutional Neural Networks)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"3-unsupervised-learning-for-robotics",children:"3. Unsupervised Learning for Robotics"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:["Clustering","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Grouping similar sensor readings"}),"\n",(0,r.jsx)(i.li,{children:"Discovering environment features"}),"\n",(0,r.jsx)(i.li,{children:"Anomaly detection"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Dimensionality Reduction","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Principal Component Analysis (PCA)"}),"\n",(0,r.jsx)(i.li,{children:"Autoencoders"}),"\n",(0,r.jsx)(i.li,{children:"Manifold learning (t-SNE, UMAP)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Feature Learning","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Learning representations from raw sensor data"}),"\n",(0,r.jsx)(i.li,{children:"Generative models (GANs, VAEs for data synthesis)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"4-reinforcement-learning-rl-fundamentals",children:"4. Reinforcement Learning (RL) Fundamentals"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:["Markov Decision Processes (MDPs)","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"States, Actions, Rewards, Transition Probabilities"}),"\n",(0,r.jsx)(i.li,{children:"Bellman Equations"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Value Functions","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"State-Value Function V(s)"}),"\n",(0,r.jsx)(i.li,{children:"Action-Value Function Q(s,a)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Policies","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Deterministic vs. Stochastic Policies"}),"\n",(0,r.jsx)(i.li,{children:"Optimal Policy"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Exploration vs. Exploitation","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Epsilon-greedy"}),"\n",(0,r.jsx)(i.li,{children:"Upper Confidence Bound (UCB)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"5-model-free-rl-algorithms",children:"5. Model-Free RL Algorithms"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:["Q-learning","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Off-policy learning"}),"\n",(0,r.jsx)(i.li,{children:"Q-table updates"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["SARSA (State-Action-Reward-State-Action)","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"On-policy learning"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Deep Q-Networks (DQN)","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Combining Q-learning with deep neural networks"}),"\n",(0,r.jsx)(i.li,{children:"Experience Replay"}),"\n",(0,r.jsx)(i.li,{children:"Target Networks"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Policy Gradients","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"REINFORCE"}),"\n",(0,r.jsx)(i.li,{children:"Actor-Critic methods (A2C, A3C)"}),"\n",(0,r.jsx)(i.li,{children:"Proximal Policy Optimization (PPO)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"6-model-based-rl-algorithms",children:"6. Model-Based RL Algorithms"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:["Learning System Dynamics","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Predicting next state from current state and action"}),"\n",(0,r.jsx)(i.li,{children:"Neural network models for dynamics"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Planning with Learned Models","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Model Predictive Control (MPC)"}),"\n",(0,r.jsx)(i.li,{children:"Monte Carlo Tree Search (MCTS)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Advantages and Disadvantages","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Sample efficiency"}),"\n",(0,r.jsx)(i.li,{children:"Model inaccuracies"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"7-imitation-learning--learning-from-demonstration-lfd",children:"7. Imitation Learning / Learning from Demonstration (LfD)"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:["Behavioral Cloning","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Supervised learning from human demonstrations"}),"\n",(0,r.jsx)(i.li,{children:"Dataset aggregation (DAgger)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Inverse Reinforcement Learning (IRL)","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Inferring reward functions from expert demonstrations"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Applications in Robotics","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Learning manipulation skills"}),"\n",(0,r.jsx)(i.li,{children:"Learning locomotion policies"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"8-continual-and-lifelong-learning",children:"8. Continual and Lifelong Learning"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:["Adapting to New Tasks and Environments","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Avoiding catastrophic forgetting"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Knowledge Transfer","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Transfer learning"}),"\n",(0,r.jsx)(i.li,{children:"Multitask learning"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Architectures for Continual Learning","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Elastic Weight Consolidation (EWC)"}),"\n",(0,r.jsx)(i.li,{children:"Progressive Neural Networks"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"9-robot-adaptation",children:"9. Robot Adaptation"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:["Online Learning","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Learning and updating models during deployment"}),"\n",(0,r.jsx)(i.li,{children:"Recursive Least Squares, Kalman Filters"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Parameter Adaptation","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Adjusting control parameters based on performance"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Self-Calibration","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Recalibrating sensors and kinematics"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(i.li,{children:"Dealing with System Changes and Wear"}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"10-challenges-in-robot-learning",children:"10. Challenges in Robot Learning"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:["Sample Efficiency","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"High cost of real-world interaction"}),"\n",(0,r.jsx)(i.li,{children:"Data augmentation and synthetic data"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Sim-to-Real Gap","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Bridging the gap between simulation and physical robots"}),"\n",(0,r.jsx)(i.li,{children:"Domain randomization"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Safety","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Ensuring safe exploration"}),"\n",(0,r.jsx)(i.li,{children:"Human-robot interaction safety"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Generalization","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Performing well in novel situations"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(i.li,{children:"Long-Horizon Tasks"}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"11-future-trends",children:"11. Future Trends"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:["Foundation Models for Robotics","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Large-scale pre-trained models"}),"\n",(0,r.jsx)(i.li,{children:"Multimodal learning"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Meta-Learning (Learning to Learn)","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Few-shot learning for rapid adaptation"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["Human-Robot Co-Learning","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Interactive learning"}),"\n",(0,r.jsx)(i.li,{children:"Shared autonomy"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(i.li,{children:"Explainable AI in Robotics"}),"\n",(0,r.jsx)(i.li,{children:"Ethical Considerations"}),"\n"]})]})}function h(n={}){const{wrapper:i}={...(0,s.R)(),...n.components};return i?(0,r.jsx)(i,{...n,children:(0,r.jsx)(c,{...n})}):c(n)}},8453:(n,i,e)=>{e.d(i,{R:()=>o,x:()=>a});var l=e(6540);const r={},s=l.createContext(r);function o(n){const i=l.useContext(s);return l.useMemo(function(){return"function"==typeof n?n(i):{...i,...n}},[i,n])}function a(n){let i;return i=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:o(n.components),l.createElement(s.Provider,{value:i},n.children)}}}]);