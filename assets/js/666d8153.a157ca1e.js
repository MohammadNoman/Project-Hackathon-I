"use strict";(globalThis.webpackChunk_001_physical_ai_textbook_docs=globalThis.webpackChunk_001_physical_ai_textbook_docs||[]).push([[5544],{7810:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"assessments/module2-assessments","title":"Module 2: Robot Sensing and Perception - Assessments","description":"Quizzes/Short Answer Questions:","source":"@site/docs/assessments/module2-assessments.md","sourceDirName":"assessments","slug":"/assessments/module2-assessments","permalink":"/Project-Hackathon-I/docs/assessments/module2-assessments","draft":false,"unlisted":false,"editUrl":"https://github.com/MohammadNoman/Project-Hackathon-I/tree/master/frontend/docs/assessments/module2-assessments.md","tags":[],"version":"current","frontMatter":{}}');var t=s(4848),o=s(8453);const r={},a="Module 2: Robot Sensing and Perception - Assessments",l={},c=[{value:"Quizzes/Short Answer Questions:",id:"quizzesshort-answer-questions",level:2},{value:"Project Prompts:",id:"project-prompts",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-2-robot-sensing-and-perception---assessments",children:"Module 2: Robot Sensing and Perception - Assessments"})}),"\n",(0,t.jsx)(n.h2,{id:"quizzesshort-answer-questions",children:"Quizzes/Short Answer Questions:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Sensor Categorization:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Question:"})," Describe the key differences between active and passive sensors, providing one example of each commonly used in robotics. Explain a scenario where one type would be preferred over the other."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Learning Outcome:"})," Understand different classifications of robot sensors."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Depth Perception:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Question:"})," Compare and contrast how stereo cameras and Time-of-Flight (ToF) cameras achieve depth perception. Discuss their respective advantages and limitations in a robotic manipulation task involving delicate objects."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Learning Outcome:"})," Differentiate between various vision systems and their depth perception mechanisms."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Lidar vs. Radar:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Question:"})," An autonomous vehicle needs to detect obstacles accurately in heavy fog and at long distances. Which sensing technology, Lidar or Radar, would be more suitable for this specific challenge and why? Elaborate on their complementary nature for robust perception."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Learning Outcome:"})," Analyze the strengths and weaknesses of Lidar and Radar in different environmental conditions."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Proprioceptive vs. Exteroceptive:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Question:"})," Explain the role of proprioceptive sensors (e.g., encoders, IMUs) in maintaining a robot's internal state knowledge, and how this data is distinct from information gathered by exteroceptive sensors (e.g., cameras, lidar). Provide an example where both types of sensors are crucial for a robot's task."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Learning Outcome:"})," Understand the function and importance of proprioceptive and exteroceptive sensors."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Image Processing Fundamentals:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Question:"})," You have a grayscale image from a robot's camera. Describe how edge detection (e.g., Canny) and morphological operations (e.g., erosion, dilation) could be used sequentially to identify and isolate a specific rectangular object with noisy boundaries."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Learning Outcome:"})," Apply basic image processing techniques for feature extraction."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"project-prompts",children:"Project Prompts:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Object Recognition and Grasping with a Simulated Robot Arm (Intermediate):"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scenario:"})," Develop a perception pipeline for a simulated robotic arm to identify and grasp specific objects (e.g., cubes, cylinders, spheres) placed on a table."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Requirements:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Utilize a simulated camera (monocular or depth camera) to acquire image/depth data."}),"\n",(0,t.jsx)(n.li,{children:"Implement an object detection algorithm (e.g., using a pre-trained YOLO model or a simpler feature-based approach like SIFT/SURF if deep learning is out of scope) to locate objects."}),"\n",(0,t.jsx)(n.li,{children:"Estimate the 6D pose of the target object."}),"\n",(0,t.jsx)(n.li,{children:"Use the estimated pose to plan and execute a grasping motion with the robot arm."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deliverables:"})," Codebase, video demonstration of the robot successfully grasping objects, a brief report explaining the perception pipeline and challenges faced."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Learning Outcomes:"})," Implement vision systems for object detection and pose estimation, apply feature extraction, and integrate perception with robotic manipulation."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Sensor Fusion for Mobile Robot Localization (Advanced):"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scenario:"})," Design and implement a sensor fusion system for a mobile robot operating in an indoor environment, aiming for accurate localization."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Requirements:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Simulate or use provided datasets for IMU data (accelerometer, gyroscope), wheel encoders (odometry), and a simulated 2D Lidar."}),"\n",(0,t.jsx)(n.li,{children:"Implement a Kalman Filter (EKF or UKF) or a Particle Filter to combine these sensor readings to estimate the robot's 2D position and orientation."}),"\n",(0,t.jsx)(n.li,{children:"Visually represent the robot's estimated path and the uncertainty ellipse/particle distribution."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deliverables:"})," Codebase, plots/visualizations of localization performance, a technical report detailing the chosen sensor fusion technique, its implementation, and performance analysis (e.g., comparing with individual sensor estimates)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Learning Outcomes:"})," Apply sensor fusion techniques (Kalman Filters, Particle Filters), understand trade-offs between different fusion methods, and evaluate localization accuracy."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Dynamic Obstacle Avoidance with Lidar and Object Tracking (Advanced):"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scenario:"})," Develop a perception and path planning system for an autonomous mobile robot to navigate an environment with both static and dynamic obstacles."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Requirements:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Utilize simulated Lidar data to detect obstacles and build a local map."}),"\n",(0,t.jsx)(n.li,{children:"Implement an object tracking algorithm (e.g., SORT or Deep SORT) to track the movement of dynamic obstacles (e.g., other robots, pedestrians)."}),"\n",(0,t.jsx)(n.li,{children:"Integrate the tracking information into a path planning algorithm that avoids both static and dynamic obstacles."}),"\n",(0,t.jsx)(n.li,{children:"Demonstrate the robot successfully navigating a cluttered environment while avoiding moving objects."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deliverables:"})," Codebase, video demonstration of the robot's navigation, a report discussing the chosen tracking and path planning algorithms, and the challenges of dynamic environments."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Learning Outcomes:"})," Process Lidar point cloud data, implement object tracking, understand challenges in dynamic environments, and integrate perception with navigation."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"AI-Driven Active Sensing Challenge (Advanced Research Project):"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scenario:"})," Explore how reinforcement learning could be used to enable a robot to actively control its sensor placement or movement to improve perception for a specific task (e.g., better identifying a partially occluded object, optimizing view for 3D reconstruction)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Requirements:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Define a task and a simulated environment where active sensing would be beneficial."}),"\n",(0,t.jsx)(n.li,{children:"Design an RL agent that can choose sensor actions (e.g., move camera, change focus) based on its current perception."}),"\n",(0,t.jsx)(n.li,{children:"Train the RL agent and demonstrate its ability to improve perception compared to a static sensing strategy."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deliverables:"})," Research paper outlining the problem, methodology, experimental setup, results, and discussion; codebase and simulation environment."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Learning Outcomes:"})," Apply reinforcement learning to active sensing, understand the concept of end-to-end learning for perception, and address challenges in computational constraints and uncertainty."]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>a});var i=s(6540);const t={},o=i.createContext(t);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);