"use strict";(globalThis.webpackChunk_001_physical_ai_textbook_docs=globalThis.webpackChunk_001_physical_ai_textbook_docs||[]).push([[9535],{4957:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"assessments/module8-assessments","title":"Module 8: Reinforcement Learning for Robotics - Assessments","description":"Quizzes/Formative Assessments","source":"@site/docs/assessments/module8-assessments.md","sourceDirName":"assessments","slug":"/assessments/module8-assessments","permalink":"/Project-Hackathon-I/docs/assessments/module8-assessments","draft":false,"unlisted":false,"editUrl":"https://github.com/MohammadNoman/Project-Hackathon-I/tree/master/frontend/docs/assessments/module8-assessments.md","tags":[],"version":"current","frontMatter":{}}');var s=i(4848),t=i(8453);const o={},a="Module 8: Reinforcement Learning for Robotics - Assessments",l={},c=[{value:"Quizzes/Formative Assessments",id:"quizzesformative-assessments",level:2},{value:"Quiz 8.1: Introduction to RL in Robotics",id:"quiz-81-introduction-to-rl-in-robotics",level:3},{value:"Quiz 8.2: Foundations of Reinforcement Learning",id:"quiz-82-foundations-of-reinforcement-learning",level:3},{value:"Quiz 8.3: Model-Free RL for Control",id:"quiz-83-model-free-rl-for-control",level:3},{value:"Quiz 8.4: Model-Based RL for Robotics",id:"quiz-84-model-based-rl-for-robotics",level:3},{value:"Quiz 8.5: Sim-to-Real Transfer &amp; Reward Function Design",id:"quiz-85-sim-to-real-transfer--reward-function-design",level:3},{value:"Project Prompts/Summative Assessments",id:"project-promptssummative-assessments",level:2},{value:"Project 8.1: Implementing and Analyzing a Basic RL Algorithm for a Robotic Task",id:"project-81-implementing-and-analyzing-a-basic-rl-algorithm-for-a-robotic-task",level:3},{value:"Project 8.2: Exploring Sim-to-Real Transfer Techniques",id:"project-82-exploring-sim-to-real-transfer-techniques",level:3},{value:"Project 8.3: Reward Engineering or Inverse Reinforcement Learning",id:"project-83-reward-engineering-or-inverse-reinforcement-learning",level:3}];function d(e){const n={em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"module-8-reinforcement-learning-for-robotics---assessments",children:"Module 8: Reinforcement Learning for Robotics - Assessments"})}),"\n",(0,s.jsx)(n.h2,{id:"quizzesformative-assessments",children:"Quizzes/Formative Assessments"}),"\n",(0,s.jsx)(n.h3,{id:"quiz-81-introduction-to-rl-in-robotics",children:"Quiz 8.1: Introduction to RL in Robotics"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Multiple Choice:"})," Which of the following is NOT a primary reason why Reinforcement Learning (RL) is suitable for robotics?\na) Handling complex control problems with non-linear dynamics.\nb) Learning directly from explicit programming instructions.\nc) Adaptability to new and changing environments.\nd) Continuous improvement through interaction with the environment."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Short Answer:"})," Briefly explain two significant challenges when applying RL to real-world robotics."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"quiz-82-foundations-of-reinforcement-learning",children:"Quiz 8.2: Foundations of Reinforcement Learning"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Fill in the Blanks:"})," In a Markov Decision Process (MDP), the agent is in a ",(0,s.jsx)(n.strong,{children:"[State]"}),", takes an ",(0,s.jsx)(n.strong,{children:"[Action]"}),", receives a ",(0,s.jsx)(n.strong,{children:"[Reward]"}),", and transitions to a new state based on ",(0,s.jsx)(n.strong,{children:"[Transition Probability]"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"True/False:"})," A deterministic policy outputs a probability distribution over actions for each state."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Concept Matching:"})," Match the following terms with their descriptions:\na) Q-function\nb) V-function\nc) Advantage Function"]}),"\n",(0,s.jsx)(n.p,{children:"i) Estimates the expected return starting from a state, taking an action, and then following a policy.\nii) Measures how much better it is to take a specific action compared to the average action.\niii) Estimates the expected return starting from a state and following a policy thereafter."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"quiz-83-model-free-rl-for-control",children:"Quiz 8.3: Model-Free RL for Control"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Compare/Contrast:"})," Differentiate between Q-learning and SARSA in terms of their on-policy/off-policy nature."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Short Answer:"}),' Explain the purpose of "Experience Replay" and "Target Networks" in Deep Q-Networks (DQN).']}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Multiple Choice:"})," Which of the following policy gradient methods is known for its stability and uses a clipped surrogate objective function to limit policy updates?\na) REINFORCE\nb) A3C\nc) PPO\nd) DDPG"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"quiz-84-model-based-rl-for-robotics",children:"Quiz 8.4: Model-Based RL for Robotics"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Short Answer:"}),' Describe the difference between a "forward model" and an "inverse model" in the context of learning dynamics models for robotics.']}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Explanation:"})," How does Model Predictive Control (MPC) utilize a learned dynamics model to control a robot?"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"quiz-85-sim-to-real-transfer--reward-function-design",children:"Quiz 8.5: Sim-to-Real Transfer & Reward Function Design"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Definition:"}),' What is the "simulation gap" in RL for robotics, and what are two common causes?']}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"True/False:"})," Domain randomization aims to perfectly match the simulation environment to the real world."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Short Answer:"})," Explain the core idea behind Inverse Reinforcement Learning (IRL) and why it's useful for reward design."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"project-promptssummative-assessments",children:"Project Prompts/Summative Assessments"}),"\n",(0,s.jsx)(n.h3,{id:"project-81-implementing-and-analyzing-a-basic-rl-algorithm-for-a-robotic-task",children:"Project 8.1: Implementing and Analyzing a Basic RL Algorithm for a Robotic Task"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective:"})," Implement a foundational model-free RL algorithm (e.g., Q-learning or a basic Policy Gradient method) for a simplified robotic control task in a simulated environment."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Task:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Choose a Task:"})," Select a simple robotic task (e.g., a pendulum swing-up, a mobile robot navigation in a grid world, or a simple arm reaching task) that can be simulated."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Environment Setup:"})," Define the state space, action space, and reward function for your chosen task. Consider if sparse or dense rewards are more appropriate and justify your choice."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Algorithm Implementation:"})," Implement either Q-learning (for discrete spaces) or a basic Policy Gradient method (e.g., REINFORCE) using a suitable function approximator (e.g., a small neural network if using policy gradients)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Training and Evaluation:"})," Train your agent in the simulated environment. Plot the learning curve (e.g., reward per episode) and evaluate the learned policy's performance."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Analysis and Discussion:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Discuss the challenges encountered during implementation and training."}),"\n",(0,s.jsx)(n.li,{children:"Analyze the impact of hyperparameter choices (e.g., learning rate, discount factor, exploration strategy) on convergence and final policy performance."}),"\n",(0,s.jsx)(n.li,{children:"Propose potential improvements to your chosen algorithm or reward function for this task."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"project-82-exploring-sim-to-real-transfer-techniques",children:"Project 8.2: Exploring Sim-to-Real Transfer Techniques"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective:"})," Investigate and demonstrate the impact of domain randomization on sim-to-real transfer for a robot learning task."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Task:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Choose a Task & Simulation:"}),' Select a simple manipulation or locomotion task. Create a basic simulation of this task (e.g., using PyBullet, MuJoCo, or a simpler custom environment). Introduce a "reality gap" by having slightly different physical parameters for your "real-world" target vs. initial simulation parameters (e.g., different friction, mass, sensor noise).']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Baseline Training:"})," Train an RL policy (e.g., DDPG or PPO) in your ",(0,s.jsx)(n.em,{children:"initial"}),', non-randomized simulation. Evaluate its performance on both the initial simulation and the "real-world" target simulation.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Domain Randomization Implementation:"})," Implement domain randomization. Randomize several key physical or visual parameters (e.g., object mass, friction coefficients, joint damping, lighting, textures) within a defined range during training."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Randomized Training & Evaluation:"}),' Train a new RL policy with domain randomization. Evaluate its performance on both the randomized simulation range and the "real-world" target simulation.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Comparative Analysis:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'Compare the performance and robustness of the baseline policy versus the domain-randomized policy on the "real-world" target simulation.'}),"\n",(0,s.jsx)(n.li,{children:"Discuss how different randomization ranges or types of parameters affect the transferability."}),"\n",(0,s.jsx)(n.li,{children:"Suggest other sim-to-real techniques (e.g., adversarial training, policy adaptation) that could further improve the transfer and explain why."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"project-83-reward-engineering-or-inverse-reinforcement-learning",children:"Project 8.3: Reward Engineering or Inverse Reinforcement Learning"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective:"})," Design and evaluate different reward functions for a robotic task, or implement a basic Inverse Reinforcement Learning (IRL) approach."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Option A: Reward Engineering"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Choose a Task:"})," Select a robotic task (e.g., reaching, grasping, balancing) where reward design is non-trivial."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Design Multiple Rewards:"})," Create at least two different reward functions for the same task: one sparse and one dense (or a shaped reward). Justify your design choices."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Train and Compare:"}),' Train the same RL algorithm (e.g., DQN, PPO) with each of your designed reward functions. Compare the learning speed, final policy performance, and any observed "reward hacking" behaviors.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Analysis:"})," Discuss the trade-offs between sparse and dense/shaped rewards, and the challenges of aligning the reward with the true objective."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Option B: Basic Inverse Reinforcement Learning (IRL)"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Choose a Simple Task:"})," Select a very simple grid-world or continuous robotic task where expert demonstrations can be easily generated."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Generate Expert Demonstrations:"})," Record a few optimal (or near-optimal) trajectories for the task."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Implement Basic IRL:"})," Implement a simple IRL algorithm (e.g., a basic feature matching or maximum entropy IRL variant, even a simplified version focusing on matching state visitation frequencies). You might need to approximate the reward function with a linear combination of features."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Infer Reward and Evaluate:"})," Use your IRL implementation to infer a reward function from the expert demonstrations. Then, train an RL agent with the ",(0,s.jsx)(n.em,{children:"inferred"})," reward function and evaluate its performance against the expert demonstrations or a known optimal policy."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Discussion:"})," Discuss the challenges of IRL, such as the ambiguity of the reward function and the need for good feature representations."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var r=i(6540);const s={},t=r.createContext(s);function o(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);