<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-assessments/module8-assessments" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Module 8: Reinforcement Learning for Robotics - Assessments | My Site</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://your-docusaurus-site.example.com/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://your-docusaurus-site.example.com/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://your-docusaurus-site.example.com/docs/assessments/module8-assessments"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Module 8: Reinforcement Learning for Robotics - Assessments | My Site"><meta data-rh="true" name="description" content="Quizzes/Formative Assessments"><meta data-rh="true" property="og:description" content="Quizzes/Formative Assessments"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://your-docusaurus-site.example.com/docs/assessments/module8-assessments"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/docs/assessments/module8-assessments" hreflang="en"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/docs/assessments/module8-assessments" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://YOUR_ALGOLIA_APP_ID-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="My Site RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="My Site Atom Feed">




<link rel="search" type="application/opensearchdescription+xml" title="My Site" href="/opensearch.xml"><link rel="stylesheet" href="/assets/css/styles.0de9cafe.css">
<script src="/assets/js/runtime~main.dfb1c517.js" defer="defer"></script>
<script src="/assets/js/main.c55686cb.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">My Site</b></a><a class="navbar__item navbar__link" href="/docs/module1-ros2-nervous-system/">Tutorial</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts="Meta+k"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 24 24" aria-hidden="true"><circle cx="11" cy="11" r="8" stroke="currentColor" fill="none" stroke-width="1.4"></circle><path d="m21 21-4.3-4.3" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><main class="docMainContainer_TBSr docMainContainerEnhanced_lQrH"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Module 8: Reinforcement Learning for Robotics - Assessments</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="quizzesformative-assessments">Quizzes/Formative Assessments<a href="#quizzesformative-assessments" class="hash-link" aria-label="Direct link to Quizzes/Formative Assessments" title="Direct link to Quizzes/Formative Assessments" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="quiz-81-introduction-to-rl-in-robotics">Quiz 8.1: Introduction to RL in Robotics<a href="#quiz-81-introduction-to-rl-in-robotics" class="hash-link" aria-label="Direct link to Quiz 8.1: Introduction to RL in Robotics" title="Direct link to Quiz 8.1: Introduction to RL in Robotics" translate="no">​</a></h3>
<ol>
<li class="">
<p><strong>Multiple Choice:</strong> Which of the following is NOT a primary reason why Reinforcement Learning (RL) is suitable for robotics?
a) Handling complex control problems with non-linear dynamics.
b) Learning directly from explicit programming instructions.
c) Adaptability to new and changing environments.
d) Continuous improvement through interaction with the environment.</p>
</li>
<li class="">
<p><strong>Short Answer:</strong> Briefly explain two significant challenges when applying RL to real-world robotics.</p>
</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="quiz-82-foundations-of-reinforcement-learning">Quiz 8.2: Foundations of Reinforcement Learning<a href="#quiz-82-foundations-of-reinforcement-learning" class="hash-link" aria-label="Direct link to Quiz 8.2: Foundations of Reinforcement Learning" title="Direct link to Quiz 8.2: Foundations of Reinforcement Learning" translate="no">​</a></h3>
<ol>
<li class="">
<p><strong>Fill in the Blanks:</strong> In a Markov Decision Process (MDP), the agent is in a <strong>[State]</strong>, takes an <strong>[Action]</strong>, receives a <strong>[Reward]</strong>, and transitions to a new state based on <strong>[Transition Probability]</strong>.</p>
</li>
<li class="">
<p><strong>True/False:</strong> A deterministic policy outputs a probability distribution over actions for each state.</p>
</li>
<li class="">
<p><strong>Concept Matching:</strong> Match the following terms with their descriptions:
a) Q-function
b) V-function
c) Advantage Function</p>
<p>i) Estimates the expected return starting from a state, taking an action, and then following a policy.
ii) Measures how much better it is to take a specific action compared to the average action.
iii) Estimates the expected return starting from a state and following a policy thereafter.</p>
</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="quiz-83-model-free-rl-for-control">Quiz 8.3: Model-Free RL for Control<a href="#quiz-83-model-free-rl-for-control" class="hash-link" aria-label="Direct link to Quiz 8.3: Model-Free RL for Control" title="Direct link to Quiz 8.3: Model-Free RL for Control" translate="no">​</a></h3>
<ol>
<li class="">
<p><strong>Compare/Contrast:</strong> Differentiate between Q-learning and SARSA in terms of their on-policy/off-policy nature.</p>
</li>
<li class="">
<p><strong>Short Answer:</strong> Explain the purpose of &quot;Experience Replay&quot; and &quot;Target Networks&quot; in Deep Q-Networks (DQN).</p>
</li>
<li class="">
<p><strong>Multiple Choice:</strong> Which of the following policy gradient methods is known for its stability and uses a clipped surrogate objective function to limit policy updates?
a) REINFORCE
b) A3C
c) PPO
d) DDPG</p>
</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="quiz-84-model-based-rl-for-robotics">Quiz 8.4: Model-Based RL for Robotics<a href="#quiz-84-model-based-rl-for-robotics" class="hash-link" aria-label="Direct link to Quiz 8.4: Model-Based RL for Robotics" title="Direct link to Quiz 8.4: Model-Based RL for Robotics" translate="no">​</a></h3>
<ol>
<li class="">
<p><strong>Short Answer:</strong> Describe the difference between a &quot;forward model&quot; and an &quot;inverse model&quot; in the context of learning dynamics models for robotics.</p>
</li>
<li class="">
<p><strong>Explanation:</strong> How does Model Predictive Control (MPC) utilize a learned dynamics model to control a robot?</p>
</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="quiz-85-sim-to-real-transfer--reward-function-design">Quiz 8.5: Sim-to-Real Transfer &amp; Reward Function Design<a href="#quiz-85-sim-to-real-transfer--reward-function-design" class="hash-link" aria-label="Direct link to Quiz 8.5: Sim-to-Real Transfer &amp; Reward Function Design" title="Direct link to Quiz 8.5: Sim-to-Real Transfer &amp; Reward Function Design" translate="no">​</a></h3>
<ol>
<li class="">
<p><strong>Definition:</strong> What is the &quot;simulation gap&quot; in RL for robotics, and what are two common causes?</p>
</li>
<li class="">
<p><strong>True/False:</strong> Domain randomization aims to perfectly match the simulation environment to the real world.</p>
</li>
<li class="">
<p><strong>Short Answer:</strong> Explain the core idea behind Inverse Reinforcement Learning (IRL) and why it&#x27;s useful for reward design.</p>
</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="project-promptssummative-assessments">Project Prompts/Summative Assessments<a href="#project-promptssummative-assessments" class="hash-link" aria-label="Direct link to Project Prompts/Summative Assessments" title="Direct link to Project Prompts/Summative Assessments" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="project-81-implementing-and-analyzing-a-basic-rl-algorithm-for-a-robotic-task">Project 8.1: Implementing and Analyzing a Basic RL Algorithm for a Robotic Task<a href="#project-81-implementing-and-analyzing-a-basic-rl-algorithm-for-a-robotic-task" class="hash-link" aria-label="Direct link to Project 8.1: Implementing and Analyzing a Basic RL Algorithm for a Robotic Task" title="Direct link to Project 8.1: Implementing and Analyzing a Basic RL Algorithm for a Robotic Task" translate="no">​</a></h3>
<p><strong>Objective:</strong> Implement a foundational model-free RL algorithm (e.g., Q-learning or a basic Policy Gradient method) for a simplified robotic control task in a simulated environment.</p>
<p><strong>Task:</strong></p>
<ol>
<li class=""><strong>Choose a Task:</strong> Select a simple robotic task (e.g., a pendulum swing-up, a mobile robot navigation in a grid world, or a simple arm reaching task) that can be simulated.</li>
<li class=""><strong>Environment Setup:</strong> Define the state space, action space, and reward function for your chosen task. Consider if sparse or dense rewards are more appropriate and justify your choice.</li>
<li class=""><strong>Algorithm Implementation:</strong> Implement either Q-learning (for discrete spaces) or a basic Policy Gradient method (e.g., REINFORCE) using a suitable function approximator (e.g., a small neural network if using policy gradients).</li>
<li class=""><strong>Training and Evaluation:</strong> Train your agent in the simulated environment. Plot the learning curve (e.g., reward per episode) and evaluate the learned policy&#x27;s performance.</li>
<li class=""><strong>Analysis and Discussion:</strong>
<ul>
<li class="">Discuss the challenges encountered during implementation and training.</li>
<li class="">Analyze the impact of hyperparameter choices (e.g., learning rate, discount factor, exploration strategy) on convergence and final policy performance.</li>
<li class="">Propose potential improvements to your chosen algorithm or reward function for this task.</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="project-82-exploring-sim-to-real-transfer-techniques">Project 8.2: Exploring Sim-to-Real Transfer Techniques<a href="#project-82-exploring-sim-to-real-transfer-techniques" class="hash-link" aria-label="Direct link to Project 8.2: Exploring Sim-to-Real Transfer Techniques" title="Direct link to Project 8.2: Exploring Sim-to-Real Transfer Techniques" translate="no">​</a></h3>
<p><strong>Objective:</strong> Investigate and demonstrate the impact of domain randomization on sim-to-real transfer for a robot learning task.</p>
<p><strong>Task:</strong></p>
<ol>
<li class=""><strong>Choose a Task &amp; Simulation:</strong> Select a simple manipulation or locomotion task. Create a basic simulation of this task (e.g., using PyBullet, MuJoCo, or a simpler custom environment). Introduce a &quot;reality gap&quot; by having slightly different physical parameters for your &quot;real-world&quot; target vs. initial simulation parameters (e.g., different friction, mass, sensor noise).</li>
<li class=""><strong>Baseline Training:</strong> Train an RL policy (e.g., DDPG or PPO) in your <em>initial</em>, non-randomized simulation. Evaluate its performance on both the initial simulation and the &quot;real-world&quot; target simulation.</li>
<li class=""><strong>Domain Randomization Implementation:</strong> Implement domain randomization. Randomize several key physical or visual parameters (e.g., object mass, friction coefficients, joint damping, lighting, textures) within a defined range during training.</li>
<li class=""><strong>Randomized Training &amp; Evaluation:</strong> Train a new RL policy with domain randomization. Evaluate its performance on both the randomized simulation range and the &quot;real-world&quot; target simulation.</li>
<li class=""><strong>Comparative Analysis:</strong>
<ul>
<li class="">Compare the performance and robustness of the baseline policy versus the domain-randomized policy on the &quot;real-world&quot; target simulation.</li>
<li class="">Discuss how different randomization ranges or types of parameters affect the transferability.</li>
<li class="">Suggest other sim-to-real techniques (e.g., adversarial training, policy adaptation) that could further improve the transfer and explain why.</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="project-83-reward-engineering-or-inverse-reinforcement-learning">Project 8.3: Reward Engineering or Inverse Reinforcement Learning<a href="#project-83-reward-engineering-or-inverse-reinforcement-learning" class="hash-link" aria-label="Direct link to Project 8.3: Reward Engineering or Inverse Reinforcement Learning" title="Direct link to Project 8.3: Reward Engineering or Inverse Reinforcement Learning" translate="no">​</a></h3>
<p><strong>Objective:</strong> Design and evaluate different reward functions for a robotic task, or implement a basic Inverse Reinforcement Learning (IRL) approach.</p>
<p><strong>Option A: Reward Engineering</strong></p>
<ol>
<li class=""><strong>Choose a Task:</strong> Select a robotic task (e.g., reaching, grasping, balancing) where reward design is non-trivial.</li>
<li class=""><strong>Design Multiple Rewards:</strong> Create at least two different reward functions for the same task: one sparse and one dense (or a shaped reward). Justify your design choices.</li>
<li class=""><strong>Train and Compare:</strong> Train the same RL algorithm (e.g., DQN, PPO) with each of your designed reward functions. Compare the learning speed, final policy performance, and any observed &quot;reward hacking&quot; behaviors.</li>
<li class=""><strong>Analysis:</strong> Discuss the trade-offs between sparse and dense/shaped rewards, and the challenges of aligning the reward with the true objective.</li>
</ol>
<p><strong>Option B: Basic Inverse Reinforcement Learning (IRL)</strong></p>
<ol>
<li class=""><strong>Choose a Simple Task:</strong> Select a very simple grid-world or continuous robotic task where expert demonstrations can be easily generated.</li>
<li class=""><strong>Generate Expert Demonstrations:</strong> Record a few optimal (or near-optimal) trajectories for the task.</li>
<li class=""><strong>Implement Basic IRL:</strong> Implement a simple IRL algorithm (e.g., a basic feature matching or maximum entropy IRL variant, even a simplified version focusing on matching state visitation frequencies). You might need to approximate the reward function with a linear combination of features.</li>
<li class=""><strong>Infer Reward and Evaluate:</strong> Use your IRL implementation to infer a reward function from the expert demonstrations. Then, train an RL agent with the <em>inferred</em> reward function and evaluate its performance against the expert demonstrations or a known optimal policy.</li>
<li class=""><strong>Discussion:</strong> Discuss the challenges of IRL, such as the ambiguity of the reward function and the need for good feature representations.</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/assessments/module8-assessments.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#quizzesformative-assessments" class="table-of-contents__link toc-highlight">Quizzes/Formative Assessments</a><ul><li><a href="#quiz-81-introduction-to-rl-in-robotics" class="table-of-contents__link toc-highlight">Quiz 8.1: Introduction to RL in Robotics</a></li><li><a href="#quiz-82-foundations-of-reinforcement-learning" class="table-of-contents__link toc-highlight">Quiz 8.2: Foundations of Reinforcement Learning</a></li><li><a href="#quiz-83-model-free-rl-for-control" class="table-of-contents__link toc-highlight">Quiz 8.3: Model-Free RL for Control</a></li><li><a href="#quiz-84-model-based-rl-for-robotics" class="table-of-contents__link toc-highlight">Quiz 8.4: Model-Based RL for Robotics</a></li><li><a href="#quiz-85-sim-to-real-transfer--reward-function-design" class="table-of-contents__link toc-highlight">Quiz 8.5: Sim-to-Real Transfer &amp; Reward Function Design</a></li></ul></li><li><a href="#project-promptssummative-assessments" class="table-of-contents__link toc-highlight">Project Prompts/Summative Assessments</a><ul><li><a href="#project-81-implementing-and-analyzing-a-basic-rl-algorithm-for-a-robotic-task" class="table-of-contents__link toc-highlight">Project 8.1: Implementing and Analyzing a Basic RL Algorithm for a Robotic Task</a></li><li><a href="#project-82-exploring-sim-to-real-transfer-techniques" class="table-of-contents__link toc-highlight">Project 8.2: Exploring Sim-to-Real Transfer Techniques</a></li><li><a href="#project-83-reward-engineering-or-inverse-reinforcement-learning" class="table-of-contents__link toc-highlight">Project 8.3: Reward Engineering or Inverse Reinforcement Learning</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>